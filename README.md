# Semaine 1 : RÃ©gression - Machine Learning Classique

## ğŸ¯ Objectifs
- Comprendre et appliquer la **rÃ©gression linÃ©aire** (simple et multiple)
- DÃ©couvrir la **rÃ©gression logistique** (binaire et multiclasse)
- Appliquer les techniques de **rÃ©gularisation** : Ridge, Lasso, ElasticNet
- Utiliser **Scikit-learn** pour entraÃ®ner, Ã©valuer et comparer des modÃ¨les
- RÃ©aliser un **mini-projet** de prÃ©diction du prix des maisons

---

## ğŸ“– Concepts ThÃ©oriques

### 1. RÃ©gression LinÃ©aire
- ModÃ¨le pour prÃ©dire une variable continue : 
 
$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n + \epsilon
$$

- Objectif : approximer la relation entre **features** et **target**.
- Utilisations : prÃ©diction du prix dâ€™une maison, prÃ©vision de ventes, etc.

### 2. RÃ©gression Logistique
- Sert Ã  prÃ©dire une **classe** (0/1 ou multiclasse) Ã  partir des features.  
- Fonction sigmoÃ¯de :  

$$
P(y=1|x) = \frac{1}{1+e^{ - ( \beta_0 + \beta_1 x)}}
$$  

- Exemples : prÃ©dire si un client va churner, survie dâ€™un passager Titanic.

### 3. RÃ©gularisation
- Ã‰vite le **surapprentissage** (overfitting) sur des datasets avec beaucoup de variables.  
- **Ridge (L2)** : pÃ©nalise les grands coefficients  
- **Lasso (L1)** : pousse certains coefficients Ã  0 (sÃ©lection de variables)  
- **ElasticNet** : combinaison de L1 et L2

---

## âš™ï¸ Scikit-learn API
| ModÃ¨le | Classe Scikit-learn |
|--------|------------------|
| RÃ©gression linÃ©aire | `LinearRegression()` |
| RÃ©gression logistique | `LogisticRegression()` |
| Ridge | `Ridge()` |
| Lasso | `Lasso()` |
| ElasticNet | `ElasticNet()` |

---

## ğŸ“ Mini-Projet : PrÃ©diction du Prix des Maisons
- **Dataset** : Boston Housing (`sklearn.datasets`) ou Kaggle House Prices
- **Ã‰tapes recommandÃ©es** :
1. Charger et explorer les donnÃ©es (`pandas`, `head()`, `describe()`)  
2. PrÃ©traitement : nettoyage, normalisation, sÃ©paration train/test  
3. EntraÃ®ner plusieurs modÃ¨les : LinearRegression, Ridge, Lasso  
4. Ã‰valuer avec **RMSE** et **RÂ²**  
5. Visualiser les rÃ©sultats (scatter plots, courbes prÃ©diction vs rÃ©alitÃ©)  

- **Extensions possibles** :
  - Tester ElasticNet et comparer aux autres modÃ¨les  
  - SÃ©lection de features importantes avec `coef_`  
  - GridSearchCV pour tuning hyperparamÃ¨tres

---

## ğŸ“š Ressources
- AurÃ©lien GÃ©ron â€“ *Hands-On Machine Learning with Scikit-Learn & TensorFlow*, Chapitre 4  
- [Scikit-learn Linear Models Documentation](https://scikit-learn.org/stable/modules/linear_model.html)  
- [Kaggle House Prices Dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)  

---

## ğŸš€ Instructions pour exÃ©cuter les notebooks
1. Installer les packages nÃ©cessaires :  
```bash
pip install numpy pandas matplotlib scikit-learn

=======
# Data Science Roadmap ğŸš€

Ce repository contient mon parcours complet pour maÃ®triser la **Data Science, Machine Learning et Deep Learning** Ã  travers des projets pratiques avec **Scikit-learn, TensorFlow et PyTorch**.

## ğŸ“‚ Contenu
- **Week 1 : RÃ©gression** â†’ rÃ©gression linÃ©aire, logistique, rÃ©gularisation (Ridge, Lasso, ElasticNet)
- **Week 2 : Arbres & Ensembles** â†’ Decision Trees, Random Forest, XGBoost, LightGBM
- **Week 3 : Clustering** â†’ K-Means, DBSCAN
- **Week 4 : RÃ©duction dimensionnelle** â†’ PCA, t-SNE
- **Week 5 : Ã‰valuation et validation** â†’ mÃ©triques, cross-validation, GridSearch
- **Week 6 : Projet ML intÃ©grateur** â†’ participation Kaggle (Titanic, House Prices)

Ensuite :
- **Deep Learning** : CNN, RNN, LSTM, Attention
- **NLP** : Word2Vec, BERT, Transformers
- **Computer Vision** : CNN avancÃ©s, YOLO, ResNet, Vision Transformers

## âš™ï¸ Technologies
- Python (NumPy, Pandas, Matplotlib, Seaborn)
- Scikit-learn
- TensorFlow / Keras
- PyTorch
- Jupyter Notebook

