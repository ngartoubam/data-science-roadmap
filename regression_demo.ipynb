{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYKbNYy0mgkA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Demo - Semaine 1"
      ],
      "metadata": {
        "id": "-fp2BkRynhNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import des librairies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
        "from sklearn.datasets import load_diabetes, load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score"
      ],
      "metadata": {
        "id": "h6Cm32iWnxMa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **numpy et pandas** → manipuler les données.\n",
        "*   **matplotlib.pyplot** → faire des graphiques.\n",
        "*   **scikit-learn (sklearn)** → contient les algorithmes et les datasets.\n",
        "       * LinearRegression, LogisticRegression, Ridge, Lasso = modèles de\n",
        "régression.\n",
        "       * load_diabetes, load_iris = datasets intégrés dans sklearn.\n",
        "       * train_test_split = séparer données en train/test.\n",
        "       * mean_squared_error, r2_score, accuracy_score = métriques d’évaluation."
      ],
      "metadata": {
        "id": "vXHVFsAJpbtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Régression linéaire (exemple avec dataset Diabetes)\n",
        "La régression linéaire est utilisée pour prédire une valeur continue (prix, salaire, progression d’une maladie…)."
      ],
      "metadata": {
        "id": "-mmqMrdmrgZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Régression linéaire ---\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lin_reg.predict(X_test)\n",
        "print(\"Régression linéaire - R² :\", r2_score(y_test, y_pred))\n",
        "print(\"MSE :\", mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm6H4WYaoLFL",
        "outputId": "6b889ed4-cf15-4b49-bfff-ca6661be4a3f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Régression linéaire - R² : 0.4526027629719195\n",
            "MSE : 2900.193628493482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Étapes :\n",
        "\n",
        "1. **Chargement des données**: load_diabetes → prédire la progression du diabète en fonction de caractéristiques médicales.\n",
        "    * X = variables explicatives (âge, IMC, taux de sucre, etc.).\n",
        "    * y = variable cible (progression de la maladie).\n",
        "2. **Découpage Train/Test** : 80% apprentissage, 20% test.\n",
        "3. **Entraînement du modèle** : lin_reg.fit(X_train, y_train).\n",
        "4. **Prédictions **: lin_reg.predict(X_test).\n",
        "5. **Évaluation** :\n",
        "    * r2_score = proportion de variance expliquée (0 à 1, plus c’est proche de 1, mieux c’est).\n",
        "    * mean_squared_error = erreur quadratique moyenne (plus petit = mieux).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pFe2jXCmr-cZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* R² (coefficient de détermination) : mesure la proportion de variance expliquée par le modèle.\n",
        "  * Ici : 0.45, donc le modèle explique environ 45% de la variance.\n",
        "* MSE (Mean Squared Error) : erreur quadratique moyenne.\n",
        "  * Ici : ≈ 2900, ce qui reflète l’écart entre les valeurs réelles et prédites.\n",
        "\n",
        "Le modèle de régression linéaire capture une partie de la relation entre les variables explicatives et la progression de la maladie (45%), mais il reste une erreur importante (MSE ≈ 2900). Cela indique que le modèle est simple et pourrait être amélioré par des techniques plus avancées (Ridge, Lasso, Random Forest, etc.)."
      ],
      "metadata": {
        "id": "aouXIaY2xyK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Régression logistique (exemple avec dataset Iris)\n",
        "La régression logistique est utilisée pour prédire une classe (ex : maladie ou pas, type de fleur…)."
      ],
      "metadata": {
        "id": "rvP79R1Ct8WH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "print(\"\\nRégression logistique - Accuracy :\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q2eOBg_t9uP",
        "outputId": "f5397b66-22b2-4839-9ce1-7f43431101f5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Régression logistique - Accuracy : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Chargement des données** : load_iris → dataset célèbre pour classifier des fleurs (3 classes : setosa, versicolor, virginica).\n",
        "    * X = longueurs et largeurs des pétales et sépales.\n",
        "    * y = type de fleur.\n",
        "2. **Découpage Train/Test** : comme avant.\n",
        "3. **Entraînement** du modèle logistique :\n",
        "    * LogisticRegression(max_iter=1000) → on augmente max_iter pour être sûr que ça converge.\n",
        "4. **Prédictions** sur X_test.\n",
        "5. **Évaluation** : accuracy_score = proportion de bonnes prédictions (valeur entre 0 et 1)."
      ],
      "metadata": {
        "id": "1EC38Sz8u3in"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* L’accuracy est la proportion de bonnes prédictions.\n",
        "* Ici : 1.0 (100%), ce qui veut dire que le modèle a parfaitement classé toutes les fleurs de l’échantillon test.\n",
        "\n",
        "* Le dataset Iris est simple et bien séparé entre classes → la régression logistique est suffisante pour obtenir une précision parfaite.\n",
        "* Cela ne veut pas dire que ce sera toujours le cas sur des données réelles, plus complexes et bruitées."
      ],
      "metadata": {
        "id": "x6LjeR65zm-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Régression Ridge et Lasso\n",
        "La régularisation permet de limiter le sur-apprentissage et d’améliorer la généralisation.\n",
        "* Ridge (L2) : réduit les coefficients trop grands, mais ne les annule pas.\n",
        "* Lasso (L1) : peut annuler certains coefficients → sélection de variables."
      ],
      "metadata": {
        "id": "XeY8YF61vc84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X_train, y_train)\n",
        "print(\"\\nRidge R² :\", ridge.score(X_test, y_test))\n",
        "\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X_train, y_train)\n",
        "print(\"Lasso R² :\", lasso.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wasUf0-jvd5t",
        "outputId": "fee34346-7627-4f51-9a24-c930d3a4f4e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ridge R² : 0.9440579987200235\n",
            "Lasso R² : 0.9044577045136054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici, on applique la régularisation :\n",
        "* Ridge (L2) → pénalise les grands coefficients mais ne les met pas à zéro.\n",
        "* Lasso (L1) → peut annuler certains coefficients → utile pour la sélection de variables.\n",
        "* alpha contrôle la force de régularisation (plus grand = plus de pénalisation).\n",
        "On utilise la même séparation X_train, X_test et la même y_train, y_test que pour la régression logistique.\n",
        "* .fit() → apprentissage.\n",
        "* .score() → retourne le R² sur les données de test."
      ],
      "metadata": {
        "id": "BzwzXPV0vkp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* alpha=1.0 : paramètre de régularisation.\n",
        "\n",
        "   * Plus alpha est grand, plus la pénalisation est forte.\n",
        "\n",
        "* Résultat : R² ≈ 0.944\n",
        "\n",
        "\n",
        "* alpha=0.1 : pénalisation plus faible ici que pour Ridge.\n",
        "\n",
        "* Résultat : R² ≈ 0.904\n",
        "\n",
        "* Ridge (0.944) a un score un peu meilleur que Lasso (0.904) dans ce cas précis.\n",
        "\n",
        "* Les deux scores sont bien supérieurs à la régression linéaire simple (≈0.45) → la régularisation améliore énormément la performance.\n",
        "\n",
        "* Cela signifie que la régularisation aide le modèle à mieux capter la relation et à éviter un sur-apprentissage dû au bruit."
      ],
      "metadata": {
        "id": "3geqtIdl0UK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Résumé\n",
        "\n",
        "* Régression linéaire → prédire une valeur continue.\n",
        "* Régression logistique → prédire une classe (binaire ou multiclasses).\n",
        "* Ridge et Lasso → ajoutent une contrainte pour éviter le sur-apprentissage et améliorer la généralisation.\n",
        "\n",
        "---\n",
        "\n",
        "* La régression linéaire prédit une valeur numérique.\n",
        "* La régression logistique prédit une classe.\n",
        "* La régularisation (Ridge/Lasso) améliore la robustesse du modèle et évite le sur-apprentissage."
      ],
      "metadata": {
        "id": "V66d3HLpwE6s"
      }
    }
  ]
}